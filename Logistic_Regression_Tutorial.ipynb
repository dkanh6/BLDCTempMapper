{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUJTSKt2RymYPSVCwIQXhM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkanh6/BLDCTempMapper/blob/main/Logistic_Regression_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Logistic Regression\n",
        "\n",
        "Unlike **Linear Regression** which is used to predict a continuous numerical value. **Logistic Regression** is used to predict discrete catagories. Specifically, it predicts the probability that a given input belongs to a certain category, making it ideal for scenarios where you need to classify objects into two distinct groups, such as spam vs. non-spam emails, Salmon vs. Sea Bass, or Disease vs. Non-Disease.\n",
        "\n",
        "In **Linear Regression** we find a line of best using Least Squares as our cost function which enables us to find $ R^2 $ and its assciated $ p $ value. If you want more details on Linear Regression in the format of a Juypter Notebook click the adjacent link [different Tutorial](https://github.com/dkanh6/Machine_Learning_Examples/blob/main/Linear_Regression_Tutorial.ipynb).In **Logistic Regression** we use Maximum Likelihood Estimation (MLE) to create a sigmoidal \"Logistic Function\" (Heaviside) which plots the probability of a an input being apart of a specific class.\n",
        "\n",
        "In a basic sense, a logistic regression only answers questions that have yes/no answers, or questtions that can be answered with 1 or 0, but can be extended to include multiple classes (Multinomal linear regression).\n"
      ],
      "metadata": {
        "id": "3bkPj2XpprCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Python ?\n",
        "\n",
        "It has become the default data science and ML language thanks to:\n",
        "\n",
        "1.   Python is open source\n",
        "2.   Simple syntax\n",
        "3.   Powerful libraries such as (Pandas: Data Manipulation, NumPy: Numerical Operations, Scikit-learn: Machine learning)\n",
        "\n",
        "Which together not only siplify the process of data preprocessing and model building (in the case of scikit down to 3 simple steps) but also provide extensive functionalities for model evaluation and optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "K9yKhTxjUm40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Plotting Data\n",
        "\n",
        "I will be be using a dataset called [Wine](https://archive.ics.uci.edu/dataset/109/wine), whcih maps thirteen continuous variables representing chemical contents of a wine to three labels, each a different winery in Italy and this tutorial takes heavy inspiration from [Source1](https://colab.research.google.com/github/crsmithdev/notebooks/blob/master/ml-logistic-regression/ml-logistic-regression.ipynb#scrollTo=EXg5RhV0Y3Et) and [Source2](https://colab.research.google.com/drive/1m9Tby3vTNXjcyDKkZrjBtR6rexvh8Bat#scrollTo=6gBqNClsQI6t)."
      ],
      "metadata": {
        "id": "g_DiZ_JIVY0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# This is the raw URL of the WINE dataset file\n",
        "url = 'https://raw.githubusercontent.com/dkanh6/Machine_Learning_Examples/main/wine.data'\n",
        "\n",
        "# Download the dataset\n",
        "r = requests.get(url);\n",
        "filename = \"wine.data\"\n",
        "\n",
        "# Save the content of the requests into a csv file\n",
        "with open(filename,'wb') as f:\n",
        "  f.write(r.content)\n",
        "\n",
        "# Define column names based on the datasets feature descriptions\n",
        "columns = ['class', 'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash',\n",
        "           'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
        "           'proanthocyanins', 'color_intensity', 'hue',\n",
        "           'od280/od315_of_diluted_wines', 'proline']\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(filename, header=None, names=columns)\n",
        "\n",
        "# Continue with data processing (Currently We are reducing the feature set down to just two: alcohol and ash)\n",
        "reduced = data[data['class'] <= 2] # Extracts only the wines with label tupes 1 or 2.\n",
        "X = reduced[['alcohol','ash']].values\n",
        "y = label_binarize(reduced['class'].values, classes=[1,2], neg_label=0, pos_label=1)[:,0] # using skikit-learn label_binarize which takes an m-length list with n possibe values (two in this case) and converts it to an mxn matrix.\n",
        "\n",
        "example = np.copy(data['class'].values)\n",
        "np.random.shuffle(example)\n",
        "example = example[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "8VEKbn7nUpMS",
        "outputId": "0e9a8056-8dc5-4971-ecfe-4dcf5012c611"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original [2 3 3 2 3 1 2 2 1 2]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "label_binarize() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bd1ba3440ae9>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Binarized:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1s vs all:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: label_binarize() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data into training and testing sets beforee going further can be done simply with train_test_split function from scikit-learn, which allows the user to specify a percentage (here 25%) to sample randomly from the data set and partiition away for testing"
      ],
      "metadata": {
        "id": "o6pMOT7ph_7_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yh4KZXz5iOpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}